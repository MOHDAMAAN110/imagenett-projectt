{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98e1a247",
   "metadata": {},
   "source": [
    "# Imagenette2 Dataset - Custom DataLoader & Modified Pooling Layers\n",
    "I worked on the assigned task to load the Imagenette2 dataset, create my own DataLoader, \n",
    "modify the pooling layers in the model, train it, and evaluate its performance.\n",
    "Keeping prints to a minimum to make the output clean.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9afb76da",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-08T12:27:37.797660Z",
     "iopub.status.busy": "2025-08-08T12:27:37.797355Z",
     "iopub.status.idle": "2025-08-08T12:27:49.706178Z",
     "shell.execute_reply": "2025-08-08T12:27:49.705559Z"
    },
    "papermill": {
     "duration": 11.94017,
     "end_time": "2025-08-08T12:27:49.707613",
     "exception": false,
     "start_time": "2025-08-08T12:27:37.767443",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "train_data = ImageFolder(root=\"/content/imagenette2/train\", transform=transform)\n",
    "val_data = ImageFolder(root=\"/content/imagenette2/val\", transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "962cb4a6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-08T12:27:49.881750Z",
     "iopub.status.busy": "2025-08-08T12:27:49.881492Z",
     "iopub.status.idle": "2025-08-08T12:28:40.266761Z",
     "shell.execute_reply": "2025-08-08T12:28:40.265975Z"
    },
    "papermill": {
     "duration": 50.442773,
     "end_time": "2025-08-08T12:28:40.294690",
     "exception": false,
     "start_time": "2025-08-08T12:27:49.851917",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed.\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = CustomCNN().to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop (1 epoch for now)\n",
    "for images, labels in train_loader:\n",
    "    images, labels = images.to(device), labels.to(device)\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(images)\n",
    "    loss = criterion(outputs, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(\"Training completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2e81cb99",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-08T12:29:02.676894Z",
     "iopub.status.busy": "2025-08-08T12:29:02.676404Z",
     "iopub.status.idle": "2025-08-08T12:36:49.817398Z",
     "shell.execute_reply": "2025-08-08T12:36:49.816504Z"
    },
    "papermill": {
     "duration": 467.200045,
     "end_time": "2025-08-08T12:36:49.847957",
     "exception": false,
     "start_time": "2025-08-08T12:29:02.647912",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10] completed.\n",
      "Epoch [2/10] completed.\n",
      "Epoch [3/10] completed.\n",
      "Epoch [4/10] completed.\n",
      "Epoch [5/10] completed.\n",
      "Epoch [6/10] completed.\n",
      "Epoch [7/10] completed.\n",
      "Epoch [8/10] completed.\n",
      "Epoch [9/10] completed.\n",
      "Epoch [10/10] completed.\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10  # or any number you want\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for images, labels in train_loader:\n",
    "        ...\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9bfb9658",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-08T12:37:25.856607Z",
     "iopub.status.busy": "2025-08-08T12:37:25.856311Z",
     "iopub.status.idle": "2025-08-08T12:42:55.513866Z",
     "shell.execute_reply": "2025-08-08T12:42:55.512852Z"
    },
    "papermill": {
     "duration": 329.770665,
     "end_time": "2025-08-08T12:42:55.568380",
     "exception": false,
     "start_time": "2025-08-08T12:37:25.797715",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Batch 50/296, Loss 1.9761\n",
      "Epoch 1/10, Batch 100/296, Loss 2.2101\n",
      "Epoch 1/10, Batch 150/296, Loss 1.9431\n",
      "Epoch 1/10, Batch 200/296, Loss 1.9894\n",
      "Epoch 1/10, Batch 250/296, Loss 2.0154\n",
      "Epoch 1 done in 33.3s — Loss: 2.0262, Acc: 27.50%\n",
      "Epoch 2/10, Batch 50/296, Loss 1.7215\n",
      "Epoch 2/10, Batch 100/296, Loss 1.6432\n",
      "Epoch 2/10, Batch 150/296, Loss 1.6732\n",
      "Epoch 2/10, Batch 200/296, Loss 1.8642\n",
      "Epoch 2/10, Batch 250/296, Loss 1.6175\n",
      "Epoch 2 done in 33.1s — Loss: 1.8361, Acc: 35.71%\n",
      "Epoch 3/10, Batch 50/296, Loss 1.8451\n",
      "Epoch 3/10, Batch 100/296, Loss 1.7683\n",
      "Epoch 3/10, Batch 150/296, Loss 1.5949\n",
      "Epoch 3/10, Batch 200/296, Loss 2.2609\n",
      "Epoch 3/10, Batch 250/296, Loss 1.5402\n",
      "Epoch 3 done in 33.4s — Loss: 1.7521, Acc: 39.29%\n",
      "Epoch 4/10, Batch 50/296, Loss 1.7073\n",
      "Epoch 4/10, Batch 100/296, Loss 1.8022\n",
      "Epoch 4/10, Batch 150/296, Loss 1.7264\n",
      "Epoch 4/10, Batch 200/296, Loss 1.7417\n",
      "Epoch 4/10, Batch 250/296, Loss 1.9036\n",
      "Epoch 4 done in 33.0s — Loss: 1.6791, Acc: 42.57%\n",
      "Epoch 5/10, Batch 50/296, Loss 1.7311\n",
      "Epoch 5/10, Batch 100/296, Loss 1.3236\n",
      "Epoch 5/10, Batch 150/296, Loss 1.6116\n",
      "Epoch 5/10, Batch 200/296, Loss 1.4839\n",
      "Epoch 5/10, Batch 250/296, Loss 1.5730\n",
      "Epoch 5 done in 32.9s — Loss: 1.5954, Acc: 46.47%\n",
      "Epoch 6/10, Batch 50/296, Loss 1.7718\n",
      "Epoch 6/10, Batch 100/296, Loss 1.7126\n",
      "Epoch 6/10, Batch 150/296, Loss 1.6956\n",
      "Epoch 6/10, Batch 200/296, Loss 1.3496\n",
      "Epoch 6/10, Batch 250/296, Loss 1.2026\n",
      "Epoch 6 done in 33.1s — Loss: 1.5071, Acc: 50.65%\n",
      "Epoch 7/10, Batch 50/296, Loss 1.5647\n",
      "Epoch 7/10, Batch 100/296, Loss 1.7235\n",
      "Epoch 7/10, Batch 150/296, Loss 1.2118\n",
      "Epoch 7/10, Batch 200/296, Loss 1.2652\n",
      "Epoch 7/10, Batch 250/296, Loss 1.2293\n",
      "Epoch 7 done in 32.8s — Loss: 1.4265, Acc: 53.38%\n",
      "Epoch 8/10, Batch 50/296, Loss 1.2487\n",
      "Epoch 8/10, Batch 100/296, Loss 1.5375\n",
      "Epoch 8/10, Batch 150/296, Loss 1.3921\n",
      "Epoch 8/10, Batch 200/296, Loss 1.7122\n",
      "Epoch 8/10, Batch 250/296, Loss 1.3442\n",
      "Epoch 8 done in 32.7s — Loss: 1.3737, Acc: 55.54%\n",
      "Epoch 9/10, Batch 50/296, Loss 1.1665\n",
      "Epoch 9/10, Batch 100/296, Loss 1.2026\n",
      "Epoch 9/10, Batch 150/296, Loss 1.2423\n",
      "Epoch 9/10, Batch 200/296, Loss 1.4532\n",
      "Epoch 9/10, Batch 250/296, Loss 1.1911\n",
      "Epoch 9 done in 32.7s — Loss: 1.3353, Acc: 57.40%\n",
      "Epoch 10/10, Batch 50/296, Loss 1.3432\n",
      "Epoch 10/10, Batch 100/296, Loss 1.3034\n",
      "Epoch 10/10, Batch 150/296, Loss 1.2871\n",
      "Epoch 10/10, Batch 200/296, Loss 1.2212\n",
      "Epoch 10/10, Batch 250/296, Loss 1.0631\n",
      "Epoch 10 done in 32.7s — Loss: 1.2896, Acc: 59.14%\n",
      "Saved model to /kaggle/working/imagenette_model.pth\n"
     ]
    }
   ],
   "source": [
    "# Cell: training (only if you need to retrain)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "num_epochs = 10\n",
    "train_losses, train_accs = [], []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    correct = total = 0\n",
    "    t0 = time.time()\n",
    "\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (preds == labels).sum().item()\n",
    "\n",
    "        if (i+1) % 50 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}, Batch {i+1}/{len(train_loader)}, Loss {loss.item():.4f}\")\n",
    "\n",
    "    epoch_loss /= len(train_loader)\n",
    "    epoch_acc = 100 * correct / total\n",
    "    train_losses.append(epoch_loss)\n",
    "    train_accs.append(epoch_acc)\n",
    "    print(f\"Epoch {epoch+1} done in {time.time()-t0:.1f}s — Loss: {epoch_loss:.4f}, Acc: {epoch_acc:.2f}%\")\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), model_path)\n",
    "print(\"Saved model to\", model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ad86e964",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-08T12:42:55.896076Z",
     "iopub.status.busy": "2025-08-08T12:42:55.895444Z",
     "iopub.status.idle": "2025-08-08T12:42:55.900311Z",
     "shell.execute_reply": "2025-08-08T12:42:55.899561Z"
    },
    "papermill": {
     "duration": 0.062259,
     "end_time": "2025-08-08T12:42:55.901443",
     "exception": false,
     "start_time": "2025-08-08T12:42:55.839184",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Replace IMG_SIZE with whatever size you used in training\n",
    "IMG_SIZE = 224  \n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "275e6133",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-08T12:43:39.625891Z",
     "iopub.status.busy": "2025-08-08T12:43:39.625231Z",
     "iopub.status.idle": "2025-08-08T12:43:39.628568Z",
     "shell.execute_reply": "2025-08-08T12:43:39.628024Z"
    },
    "papermill": {
     "duration": 0.060561,
     "end_time": "2025-08-08T12:43:39.629720",
     "exception": false,
     "start_time": "2025-08-08T12:43:39.569159",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dir = \"/kaggle/input/imagenette2-320/imagenette2-320/train\"\n",
    "test_dir  = \"/kaggle/input/imagenette2-320/imagenette2-320/val\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f57cf376",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-08T12:43:39.737449Z",
     "iopub.status.busy": "2025-08-08T12:43:39.736897Z",
     "iopub.status.idle": "2025-08-08T12:43:46.618710Z",
     "shell.execute_reply": "2025-08-08T12:43:46.617831Z"
    },
    "papermill": {
     "duration": 6.937163,
     "end_time": "2025-08-08T12:43:46.620324",
     "exception": false,
     "start_time": "2025-08-08T12:43:39.683161",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Transforms\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Datasets\n",
    "train_dataset = datasets.ImageFolder(train_dir, transform=train_transform)\n",
    "test_dataset  = datasets.ImageFolder(test_dir, transform=test_transform)\n",
    "\n",
    "# Dataloaders\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader  = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f4f1b08c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-08T12:43:46.731351Z",
     "iopub.status.busy": "2025-08-08T12:43:46.731047Z",
     "iopub.status.idle": "2025-08-08T12:43:46.735096Z",
     "shell.execute_reply": "2025-08-08T12:43:46.734487Z"
    },
    "papermill": {
     "duration": 0.060249,
     "end_time": "2025-08-08T12:43:46.736152",
     "exception": false,
     "start_time": "2025-08-08T12:43:46.675903",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "af70e858",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-08T12:43:46.848593Z",
     "iopub.status.busy": "2025-08-08T12:43:46.848325Z",
     "iopub.status.idle": "2025-08-08T12:43:46.852765Z",
     "shell.execute_reply": "2025-08-08T12:43:46.852216Z"
    },
    "papermill": {
     "duration": 0.061105,
     "end_time": "2025-08-08T12:43:46.853926",
     "exception": false,
     "start_time": "2025-08-08T12:43:46.792821",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                         [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                         [0.229, 0.224, 0.225])\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "77fa0452",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-08T12:43:46.965189Z",
     "iopub.status.busy": "2025-08-08T12:43:46.964964Z",
     "iopub.status.idle": "2025-08-08T12:43:51.776146Z",
     "shell.execute_reply": "2025-08-08T12:43:51.775339Z"
    },
    "papermill": {
     "duration": 4.867758,
     "end_time": "2025-08-08T12:43:51.777745",
     "exception": false,
     "start_time": "2025-08-08T12:43:46.909987",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data = datasets.ImageFolder(train_dir, transform=train_transform)\n",
    "val_data   = datasets.ImageFolder(test_dir, transform=val_transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c4ff10b9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-08T12:43:51.888804Z",
     "iopub.status.busy": "2025-08-08T12:43:51.888501Z",
     "iopub.status.idle": "2025-08-08T12:43:51.892767Z",
     "shell.execute_reply": "2025-08-08T12:43:51.892149Z"
    },
    "papermill": {
     "duration": 0.060915,
     "end_time": "2025-08-08T12:43:51.893839",
     "exception": false,
     "start_time": "2025-08-08T12:43:51.832924",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "val_loader   = DataLoader(val_data, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f7f468a5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-08T12:43:52.002244Z",
     "iopub.status.busy": "2025-08-08T12:43:52.001971Z",
     "iopub.status.idle": "2025-08-08T12:43:52.611161Z",
     "shell.execute_reply": "2025-08-08T12:43:52.610249Z"
    },
    "papermill": {
     "duration": 0.66409,
     "end_time": "2025-08-08T12:43:52.612552",
     "exception": false,
     "start_time": "2025-08-08T12:43:51.948462",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
      "100%|██████████| 44.7M/44.7M [00:00<00:00, 173MB/s]\n"
     ]
    }
   ],
   "source": [
    "model = models.resnet18(pretrained=True)\n",
    "model.fc = nn.Linear(model.fc.in_features, len(train_data.classes))\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e3d2cda5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-08T12:43:52.719996Z",
     "iopub.status.busy": "2025-08-08T12:43:52.719405Z",
     "iopub.status.idle": "2025-08-08T12:43:52.724144Z",
     "shell.execute_reply": "2025-08-08T12:43:52.723345Z"
    },
    "papermill": {
     "duration": 0.059143,
     "end_time": "2025-08-08T12:43:52.725465",
     "exception": false,
     "start_time": "2025-08-08T12:43:52.666322",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "63c45538",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-08T12:43:52.833844Z",
     "iopub.status.busy": "2025-08-08T12:43:52.833609Z",
     "iopub.status.idle": "2025-08-08T12:47:55.794374Z",
     "shell.execute_reply": "2025-08-08T12:47:55.793440Z"
    },
    "papermill": {
     "duration": 243.071671,
     "end_time": "2025-08-08T12:47:55.851583",
     "exception": false,
     "start_time": "2025-08-08T12:43:52.779912",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3], Loss: 0.6323\n",
      "Epoch [2/3], Loss: 0.3933\n",
      "Epoch [3/3], Loss: 0.2935\n"
     ]
    }
   ],
   "source": [
    "epochs = 3\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{epochs}], Loss: {running_loss/len(train_loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e41d4631",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-08T12:48:33.230449Z",
     "iopub.status.busy": "2025-08-08T12:48:33.230212Z",
     "iopub.status.idle": "2025-08-08T13:02:53.934927Z",
     "shell.execute_reply": "2025-08-08T13:02:53.934170Z"
    },
    "papermill": {
     "duration": 860.760325,
     "end_time": "2025-08-08T13:02:53.936192",
     "exception": false,
     "start_time": "2025-08-08T12:48:33.175867",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.2121, Train Accuracy: 93.38%\n",
      "Validation Accuracy: 79.72%\n",
      "Epoch [2/10], Loss: 0.1758, Train Accuracy: 94.10%\n",
      "Validation Accuracy: 83.03%\n",
      "Epoch [3/10], Loss: 0.1393, Train Accuracy: 95.43%\n",
      "Validation Accuracy: 85.48%\n",
      "Epoch [4/10], Loss: 0.1148, Train Accuracy: 96.34%\n",
      "Validation Accuracy: 84.89%\n",
      "Epoch [5/10], Loss: 0.1092, Train Accuracy: 96.54%\n",
      "Validation Accuracy: 85.91%\n",
      "Epoch [6/10], Loss: 0.0733, Train Accuracy: 97.67%\n",
      "Validation Accuracy: 87.69%\n",
      "Epoch [7/10], Loss: 0.0983, Train Accuracy: 96.90%\n",
      "Validation Accuracy: 86.19%\n",
      "Epoch [8/10], Loss: 0.0918, Train Accuracy: 97.18%\n",
      "Validation Accuracy: 88.92%\n",
      "Epoch [9/10], Loss: 0.0650, Train Accuracy: 97.82%\n",
      "Validation Accuracy: 82.60%\n",
      "Epoch [10/10], Loss: 0.0684, Train Accuracy: 97.86%\n",
      "Validation Accuracy: 88.84%\n"
     ]
    }
   ],
   "source": [
    "# Number of epochs increased\n",
    "num_epochs = 10  \n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "    train_accuracy = 100.0 * correct / total\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], \"\n",
    "          f\"Loss: {running_loss/len(train_loader):.4f}, \"\n",
    "          f\"Train Accuracy: {train_accuracy:.2f}%\")\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = outputs.max(1)\n",
    "            val_total += labels.size(0)\n",
    "            val_correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "    val_accuracy = 100.0 * val_correct / val_total\n",
    "    print(f\"Validation Accuracy: {val_accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6eba5000",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-08T13:02:54.050216Z",
     "iopub.status.busy": "2025-08-08T13:02:54.049938Z",
     "iopub.status.idle": "2025-08-08T13:02:54.054897Z",
     "shell.execute_reply": "2025-08-08T13:02:54.054033Z"
    },
    "papermill": {
     "duration": 0.063752,
     "end_time": "2025-08-08T13:02:54.056220",
     "exception": false,
     "start_time": "2025-08-08T13:02:53.992468",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                         [0.229, 0.224, 0.225])\n",
    "])\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
